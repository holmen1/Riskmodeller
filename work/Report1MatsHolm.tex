\documentclass[11pt]{article}
%\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\usepackage[a4paper,hmargin=3cm,vmargin=5cm]{geometry}
%\geometry{a4paper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb, amsmath}
\usepackage{epstopdf}
\usepackage{float}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{Report I - MT7027 Risk Models and Reserving in Non-Life Insurance}
\author{Mats Holm nnnnnn-nnnn }
\date{8 March, 2019}                                           % Activate to display a given date or no date

\begin{document}
\maketitle
\section*{Project I}
\subsection*{Objectives}
({\it Instructions: highlight the project objectives})
\\
In this project we analyze claim
arrival times and claim size distributions for two insurance products.
We will model
the distribution of the total claim cost for claim arrivals from today until one year from
today in case of no reinsurance arrangement...

\begin{align} \label{eq:sum1}
	S_1 = \sum_{k=1}^{N_1}  X_k   
\end{align}
and
\begin{align} \label{eq:sum2}
	 S_2 = \sum_{k=1}^{N_2} Y_k
\end{align}

With cumulative distribution of $S = S_1 + S_2$ find
\begin{align*}
	  \mathrm{P}\left(S < s \right). 
\end{align*}


\subsection*{Mathematical Background}
({\it Instructions: provide a mathematical background by explaining the key concepts used in your analysis})

%%% CLAIM ARRIVAL
\subsection*{Claim arrival process}
By labeling each ClaimDay between 1-365 and plot the histogram we see in Figure \ref{fig:samplefig1} we see that both ClaimTypes show a common decrease during 'summer'. This requires an inhomogenuos process. We chose

\begin{align}\label{eq:N}
	N_t  \sim  \mathrm{Pois}(\mu(t))
\end{align}
In \eqref{eq:N} we chose $\mu(t) = \lambda t$. With $\lambda$ for different claim types and season. Found with the help of R $fitdistr$-function.


\begin{table}[!ht]
\center
\begin{tabular}{r|rr}
Type & Winter & Summer \\ 
\hline
1 & $\lambda=$10.9 & $\lambda=$3.06 \\
2 & $\lambda=$6.77 & $\lambda=$2.1\\
\hline
\end{tabular}
\caption{Sample table. Use informative captions.} \label{tab:sampletab}
\end{table}

 \begin{figure}[H]
 \center
  \includegraphics[scale=.5]{histyear.png}
  \caption{Summer slowdown}
  \label{fig:samplefig1}
\end{figure}


%%% CLAIM COST
\subsection*{Claim cost}
First,  we see in Figure \ref{fig:samplefig2} we see that both ClaimTypes show no apparent seasonal variation.

ClaimType 1 lognornam
ClaimType 2 weibull
 Figure \ref{fig:samplefig3} we see that both ClaimTypes show no apparent seasonal variation.

\begin{align}\label{eq:N}
	f_{X}(x) = \dfrac{1}{\sqrt{2\pi}\sigma x} \exp\left(-\dfrac{(\log x - \mu)^2}{2\sigma^2}\right) , 
\end{align}
for all i.i.d. $X_k$ in (\ref{eq:sum1}).

\begin{align}\label{eq:N}
	f_{Y}(y) = c \tau y^{\tau - 1}e^{-cy^\tau} , 
\end{align}
for all i.i.d. $Y_k$ in (\ref{eq:sum2}).

\begin{table}[!ht]
\center
\begin{tabular}{|r|rr|}
\hline
1 LogNorm & $\mu log=$10.08248 ,& $\sigma log=$0.3247564 \\
2 Weibull & $\tau=$1.148278 ,& $c=$105523\\
\hline
\end{tabular}
\caption{Sample table. Use informative captions.} \label{tab:cost}
\end{table}

 \begin{figure}[H]
 \center
  \includegraphics[scale=.5]{meancost.png}
  \caption{Daily meancost}
  \label{fig:samplefig2}
\end{figure}

 \begin{figure}[H]
 \center
  \includegraphics[scale=.5]{histfit.png}
  \caption{Daily meancost}
  \label{fig:samplefig3}
\end{figure}

With these assumptions we can 
 create random samples of a yearly cost by

First $\hat{N} <-  sum(rpois(lambdaw,winterdays)) +  sum(rpois(lambdas,summerdays))$
which we use in

then
\begin{align*} 
	\hat{X} <- rlnorm(\hat{N}_1,\mu log,\sigma log) 
\end{align*}
and
\begin{align*} 
	 \hat{Y} <- rweibull(\hat{N}_2,\tau,c)
\end{align*}


then we model total claim cost after one year by
\begin{align} \label{eq:sum1}
	\hat{S_1} = \sum_{k=1}^{\hat{N}_1} \hat{ X}_k   
\end{align}
and
\begin{align} \label{eq:sum2}
	 \hat{S_2} = \sum_{k=1}^{\hat{N}_2} \hat{Y}_k
\end{align}


\subsection*{Dependence}
Before creating cumulative distribution, we check dependence between historical data
 Figure \ref{fig:samplefig4} showing dependence.


 \begin{figure}[H]
 \center
  \includegraphics[height=8cm, width=10cm]{rhodaily.png}
  \caption{Daily cost 10Y}
  \label{fig:samplefig4}
\end{figure}

 \begin{figure}[H]
 \center
  \includegraphics[height=8cm, width=10cm]{rhoyearly.png}
  \caption{Yearly cost 10Y}
  \label{fig:samplefig10y}
\end{figure}



%%% CUMULATIVE
\subsection*{Bivariate total claims cost}

%% Bootstrap
\subsubsection*{Bootstrap}





%% Copula
\subsubsection*{Copula}
With the help of R 
First create normar bivariate $Z_1,Z_2$.
\begin{align} \label{eq:bi1}
	 \begin{bmatrix} Z_1 \\ Z_2\end{bmatrix} \sim N\left(\begin{bmatrix} 0 \\ 0\end{bmatrix},\begin{bmatrix} 1&\rho \\ \rho&1\end{bmatrix}\right)
\end{align}
then from   $\left(\Phi(Z_1),\Phi(Z_2)\right) \sim  U[0,1] \times U[0,1]$ .
With $\hat{F}_1^\leftharpoonup$ and $\hat{F}_2^\leftharpoonup$ the inverse empirical cdf from $\hat{S_1}$ and $\hat{S_2}$ respectively.
We get
\begin{align} \label{eq:bi3}
	  \left(\tilde{S_1},\tilde{S_2}\right) = \left(\hat{F}_X^\leftharpoonup(\Phi(Z_1)),\hat{F}_Y^\leftharpoonup(\Phi(Z_2))\right) 
\end{align}
a Gaussian copula $ \left(\tilde{S_1},\tilde{S_2}\right)$!

 \begin{figure}[H]
 \center
  \includegraphics[scale=.5]{pairs5.png}
  \caption{Gaussian Copula}
  \label{fig:samplefig5}
\end{figure}

\subsection*{XL covers}

\subsection*{SL covers}

%%% STOPLOSS
\subsection*{SL cover}
Let
\begin{align*} 
	tilde{S}= tilde{S_1} + \tilde{S_2}
\end{align*}
model total cost.

Then, with ... we find the 10\%-quantile
\begin{align*} 
	K = \hat{F}_{12}^\leftharpoonup(0.9)
\end{align*}
where $\hat{F}_{12}^\leftharpoonup$ is the inverse empirical cdf for $\tilde{S}$  formed from copula $(\tilde{S_1}, \tilde{S_2})$.

\begin{align} \label{eqsl1}
	R_{SL} = \dfrac{1}{N}\sum_{n=1}^N\left(	 \tilde{S}_n-K\right)_+        
\end{align}
and then price $\pi = 1.1R_{SL} = 18507884$.

\subsection*{Summary}
({\it Summarize your results and state your conclusions. })



\begin{thebibliography}{99}
\bibitem{Dupire-94}
B. Dupire (1994),
Pricing with a smile.
\emph{Risk}, 7, 18-20.
\bibitem{Wuthrich-Merz-13}
  Mario V. W\"{u}thrich and Michael Merz (2013),
  \emph{Financial Modeling, Actuarial Valuation and Solvency in Insurance},
  Springer-Verlag Berlin Heidelberg.
\end{thebibliography}


\section*{Appendix}

\begin{verbatim}

#library(ChainLadder)
#library(xtable)


claims <- read.table("Projekt2_Grupp8.txt", header = TRUE, sep = ";")
summary(claims)
head(claims, n=10)

sub.2 <- subset(claims, is.element(ClaimType, 2), select=c(ClaimDay, PaymentDay, ClaimCost))

sub.2$ClaimYear <- sub.2$ClaimDay %/% 366
sub.2$PaymentYear <- sub.2$PaymentDay %/% 366

sub.2.yearly <- aggregate(ClaimCost ~ ClaimYear + PaymentYear, data=sub.2, FUN=sum)
sub.2.yearly$Development <- sub.2.yearly$PaymentYear - sub.2.yearly$ClaimYear

#
sub.2.yearly <- subset(sub.2.yearly, ClaimYear > 9)
triangle.2 <- incr2cum(as.triangle(sub.2.yearly,
                                   origin="ClaimYear",
                                   dev="Development",
                                   value="ClaimCost"), na.rm = FALSE)

mack.2 <- MackChainLadder(triangle.2, est.sigma="Mack")
mack.2
mack.2$f
mack.2$FullTriangle
plot(mack.2)


# Chainladder
n <- 10
f <- sapply(1:(n-1),
               function(i){
                 sum(triangle.2[c(1:(n-i)),i+1])/sum(triangle.2[c(1:(n-i)),i])
               }
)

sigma <- sapply(1:(n-1),
            function(i){
              sum(triangle.2[c(1:(n-i)),i]*(triangle.2[c(1:(n-i)),i+1]/triangle.2[c(1:(n-i)),i] - f[i])^2)/(n-i-1)
            }
)



# Ex 1.3

n_val<-1000
runs<-10000
res_mat2<-matrix(0,runs,3)
for(i in (1:runs))
{
	z_val<-rnorm(1)
	z_vec<-rnorm(n_val)
	res_mat2[i,1]<-sum(exp(z_vec))
	res_mat2[i,3]<-sum(exp(0.1*z_val+sqrt(1-0.1^2)*z_vec))
}

par(mfrow=c(1,2))
hist(res_mat2[,1],75,xlab="",ylab="",main="rho=0",xlim=c(min(res_mat2),max(res_mat2)),
col="black")
hist(res_mat2[,3],75,xlab="",ylab="",main="rho=0.1",xlim=c(min(res_mat2),max(res_mat2)),
col="black")
\end{verbatim}


\end{document}  